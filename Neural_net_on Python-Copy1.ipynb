{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                          NEURAL NETWORK\n",
    "\n",
    "![NeuralNetwork](Neuralnetwor.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating sigmoid function for each neuron**\n",
    "\n",
    "First of all, creating a function to map any value from 0 to 1. This function is called sigmoid. This function will be running on every Neuron on our neural network when data enters the network. Using it here to create probability out of numbers. \n",
    "\n",
    "[Sigmoid function wiki](https://en.wikipedia.org/wiki/Sigmoid_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sigmoid](img/sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear(x,deriv=False):\n",
    "    if(deriv--True):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    return 1/(1*np,exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**creating input data a matrix**\n",
    "\n",
    "Here each row is a different training example and each column represents differnt neuron. Here we have four training example with three input neuron each.\n",
    "\n",
    "**Input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1], \n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1], \n",
    "              [1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0utput data**\n",
    "\n",
    "Four examples and one output neuron each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seeding**\n",
    "\n",
    "We will be generating random number and here seeding them to make it [Deterministic](https://en.wikipedia.org/wiki/Deterministic_system). So that we get same sequence everytime we run our program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Synapse matrices**\n",
    "\n",
    "[Synapses](https://en.wikipedia.org/wiki/Synaptic_weight) refers to the strength or amplitude of a connection between two neurons.Each synapse is random weight assigned to it. \n",
    "\n",
    "Since we have three layers in out network we need 2 synapse matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "synap0=2*np.random.random((3,4))-1\n",
    "synap1=2*np.random.random((4,1))-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "       Here we are creating a for loop which iterate over the training code to optimize the netwrok for the given data set.\n",
    "\n",
    "  First Layer is our input data then our prediction step for that we perform [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) between each layers and its synapse. After that, we will run our sigmoid function from above on all the values in matirs to create the next layer.\n",
    "  \n",
    "  The next layer contains a prediction of the output data.Then, we do same thing on that layer to get next layer whcih is a more refined prediction.  \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60000):\n",
    "    l0=X\n",
    "    l1=nonlinear(np.dot(l0,synap0))\n",
    "    l2=nonlinear(np.dot(l1,synap1))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get error rate**\n",
    "\n",
    "So now we have a prediction of the output value in layer two, lets compare it to the expected output data using subtration to get the error rate. We are also printing the average error rate at a set interval to make it goes down every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_error = y-l2\n",
    "\n",
    "if (i%10000)==0:\n",
    "    print (\"Error:\" + str(np.mean(np.abs(l2_error))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Backpropagation**](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "\n",
    "Next, we will multiply out error rate by the result of our sigmoid function. the function is used to get the derivative of our output predition from layer 2. This will give us a delta which we will use to reduce the error rate of our prediction when we update our synapses every iteration. Then we'll want to see how much layer one contributed to the error in layer two. We get this error by multiplying layer two's delta by snapse one's transpose. Then we get layer one's delta by multiplying it's error by the result of our sigmoid function. The fucntion is to get the derivative of layer one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_delta = l2_error*nonlinear(l2, deriv=True)\n",
    "l1_error = l2_delta.dot(synap1.T)\n",
    "l1_delta = l1_error*nonlinear(l1,deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Gradient decent**](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "Now that we have deltas for each of our layers, we can use them to update our synapse rates to reduce the error rate more and more every iteration.for this, we'll jsut multiply each layer by a delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "synap1 +=l1.T.dot(l2_delta)\n",
    "synap0 +=l0.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**print Pridecicted output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.24826494]\n",
      " [ -0.59095121]\n",
      " [ -5.29565438]\n",
      " [-25.74252867]]\n"
     ]
    }
   ],
   "source": [
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24826494]\n",
      " [ 1.59095121]\n",
      " [ 6.29565438]\n",
      " [25.74252867]]\n"
     ]
    }
   ],
   "source": [
    "print(l2_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
